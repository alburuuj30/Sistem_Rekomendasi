# -*- coding: utf-8 -*-
"""Rekomendasi Buku berdasarkan Rating.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZsaLHQF9AMm91rodrA7zgQYP5IRB54mv

* Nama Lengkap : Fajar Maulana Thaariq A
* Username : alburuuj
* Email : fajarm643@gmail.com
* No Telp : +6289619785254
* Kota Domisili : Kab. Semarang
* Tempat Lahir : Kab. Boyolali
* Tanggal Lahir : 30 Juni 2001
* Pendidikan Terakhir : SMA
* Pekerjaan / Profesi saat ini : Pelajar / Mahasiswa
* Perusahaan/ Institusi saat ini : Institut Teknologi Telkom Purwokerto
"""

import pandas as pd
import numpy as np
import tensorflow as tf
import seaborn as sns
import matplotlib.pyplot as plt
import keras

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score

buku = pd.read_csv('/content/Books.csv')
buku

rating = pd.read_csv('/content/Ratings.csv')
rating

"""# Data Understanding

Mencatat data - data variable yang tersedia di dalam dataset



1.   Books
  * ISBN : Kode unik dari identitas sebuah buku
  * Book-Title : Judul dari buku
  * Book-Author : Penulis atau Pengarang buku
  * Year-of-Publication : Tahun diterbitkannya buku
  * Publisher : Penerbit Buku
  * Image-URL-S : Tautan Link sampul buku (kecil)
  * Image-URL-M : Tautan link sampul buku (Sedang)
  * Image-URL-L : Tautan link sampul buku (Besar)


2.   Ratings
  * User-ID : Nomor identitas pengguna
  * ISBN : Kode unik dari identitas sebuah buku
  * Book-Rating : Skor penilaian dari masing - masing buku

# Checking Data Information
"""

buku.info()

daftar_buku = buku['Book-Title'].value_counts().keys()
total = buku['Book-Title'].value_counts()

book_count = pd.DataFrame({'Judul Buku': daftar_buku, 'Total': total}).reset_index(drop=True)
book_count

daftar_pengarang = buku['Book-Author'].value_counts().keys()
total = buku['Book-Author'].value_counts()

author_count = pd.DataFrame({'Pengarang': daftar_pengarang, 'Total': total}).reset_index(drop=True)
author_count

print('Total missing value in dataframe:', buku.isnull().sum().sum(), 'records')

buku.isnull().sum()

"""Diketahui bahwa terdapat nilai kosong yang berada pada kolom Book-Autor, Publisher, dan Image-URL-L"""

rating.info()

daftar_nilai = rating['Book-Rating'].value_counts().keys()
total = rating['Book-Rating'].value_counts()

rating_count = pd.DataFrame({'Penilaian': daftar_nilai, 'Total': total}).reset_index(drop=True)
rating_count

"""**Checking Missing Value on Ratings.csv**"""

print('Total missing value in dataframe:', rating.isnull().sum().sum(), 'records')

"""Dihasilkan bahwa data rating tidak memiliki nilai yang kosong"""

sns.barplot(data=rating_count, x='Penilaian', y='Total')
plt.show()

"""Grafik visualisasi diatas menunjukkan bahwa skla rating terdiri dari nilai 0 sampai 10.

# Checking Duplicating on Data
"""

for col in buku.columns:
  print(f'{col}: {buku[col].duplicated().sum()}')

"""Dari 8 kolom yang tersedia, Kolom ISBN sendiri yang tidak memiliki duplikasi data. Berbeda dengan kolom yang lain yang memiliki duplikasi yang berbeda - beda"""

for col in rating.columns:
  print(f'{col}: {rating[col].duplicated().sum()}')

"""Pada data Ratings.csv semua kolom memiliki duplikasi dengan nilai yang berbeda - beda

# Content Based Filltering
---
## Data Preparation

**Menghilangkan kolom yang tidak diperlukan**
Untuk pembuatan sistem rekomendasi ini, diperlukan data dari author (Pengarang) dan rating (Penilaian) sebagai acuan dalam pembuatan model. selanjutnya, untuk kolom yang tidak lagi diperlukan akan kita hapus sehinnga tidak mengganggu ketia per modelan sedang berlangsung.
"""

unused_columns = ['Year-Of-Publication', 'Publisher', 'Image-URL-M', 'Image-URL-L']
buku.drop(unused_columns, axis=1, inplace=True)
buku

"""# Data Aggregation

Penggabungan data dari data buku dan juga data rating
"""

rating_new = rating.merge(buku,on='ISBN')
rating_new = rating_new.groupby('Book-Title').sum()['Book-Rating'].reset_index()
rating_new.rename(columns={'Book-Rating':'Num-Ratings'}, inplace=True)

book_new = pd.DataFrame({'Book-Title': buku['Book-Title'].unique()})
book_new = pd.merge(book_new, rating_new, on='Book-Title', how='left')
book_new = book_new.merge(buku, on='Book-Title').drop_duplicates('ISBN')

book_new

rating_new

"""## Menghilangkan Duplikasi Data"""

book_new = book_new.drop_duplicates('Book-Title').reset_index(drop=True)
len(book_new['ISBN'].unique()), len(book_new['Book-Title'].unique())

"""## Checking Missing Values"""

book_new.isnull().sum()

"""Kolom Num-Ratings dan Book-Author masih terdapat nilai yang kosong

## Mengatasi Missing Values
"""

book_new = book_new.dropna()
book_new.shape

"""## Checking Missing Values Again"""

book_new.isnull().sum()

"""Dari hasil daiatas, diketahui bahwa sudah tidak ada lagi kolom yang memiliki nilai kosong

## Data Selection
Melakukan seleksi data dari data yang akan digunakan selanjutnya. data yang digunakan memiliki kriteria penilaian diatas skor 60
"""

book_final = book_new[book_new['Num-Ratings'] > 60]
book_final.drop(['ISBN', 'Num-Ratings'], axis=1, inplace=True)
book_final

"""# Data SElectioan

**TFID Vectorizer**
"""

data = book_final
data.sample(3)

tfid = TfidfVectorizer(token_pattern=r"(?u)\b\w\w+\b\s+\w+")
tfid.fit(data['Book-Author']) 

tfid.get_feature_names()

"""## Merubah data ke dalam bentuk matriks"""

tfidf_matrix = tfid.fit_transform(data['Book-Author']) 
tfidf_matrix.shape

tfidf_matrix.todense()

"""## Menghitung Cosine Similarity"""

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

cosine_sim_df = pd.DataFrame(cosine_sim, index=data['Book-Title'], columns=data['Book-Title'])
cosine_sim_df

"""## Get Book Reccomendation

Mendapat rekomendasi buku berdasarkan pengarang yang sama dan history dari pembaca
"""

def recommendation_book(book_name, similarity_data=cosine_sim_df, items=data, k=5):
  index = similarity_data[book_name].to_numpy().argpartition(range(-1, -(k+1), -1))[::-1]
  closest = similarity_data.columns[index[:k+1]]
  closest = closest.drop(book_name, errors='ignore')

  return pd.DataFrame(closest).merge(items).head(k)

reference_book = 'The Testament'
data[data['Book-Title'].eq(reference_book)]

recommendation_book(reference_book, k=7)

"""# Evaluation

Dari hasil data yang usdah ditampilkan, Implementasi dari teknik Content-Based Filtering memiliki tingkat keakuratan yang sempurna dengan perekomendasian berdasarkan author(pengarang) dari buku. Dari 10 buku yang ditampilkan, seluruhnya ditampilkan berdasarkan sesuai dengan author(pengarang) dari buku yang telah ditentukan *The Tastament*. Dengan demikian dapat dikatakan tingkat akurasi ini mencapai 10/10 atau 100%.

# Collaborative Filtering
---
## Data Preparation

**Data Aggregation**
"""

dAgg = rating
dAgg = dAgg.merge(book_new, on='ISBN')
dAgg.drop(['Num-Ratings', 'Book-Author'], axis=1, inplace=True)
dAgg

"""## Encode Features
Mengubah  fitur User-ID dan Bookk-Title ke bentuk Index
"""

user_ids = dAgg['User-ID'].unique().tolist()
user2encoded = {x: i for i, x in enumerate(user_ids)}
encoded2user = {i: x for i, x in enumerate(user_ids)}

book_isbns = dAgg['ISBN'].unique().tolist()
book2encoded = {x: i for i, x in enumerate(book_isbns)}
encoded2book = {i: x for i, x in enumerate(book_isbns)}

dAgg['User-Encoded'] = dAgg['User-ID'].map(user2encoded)
dAgg['Book-Encoded'] = dAgg['ISBN'].map(book2encoded)

user_num = len(user2encoded)
print(user_num)
 
book_num = len(encoded2book)
print(book_num)

dAgg['Book-Rating'] = dAgg['Book-Rating'].values.astype(np.float32)
 
min_rating = min(dAgg['Book-Rating'])
max_rating = max(dAgg['Book-Rating'])

print(f'Number of User: {user_num}, Number of Books: {book_num}, Min Rating: {min_rating}, Max Rating: {max_rating}')
dAgg

"""## Normalizatioan Data on Rating
Melakukan transformasi dengan menggunakan Library MinMAxScaler dari fitur ke sebuah rentang yang tertentu
"""

x = dAgg[['User-Encoded', 'Book-Encoded']].values
y = dAgg['Book-Rating'].values
y = y.reshape(-1, 1)

scaler = MinMaxScaler()
norm_y = scaler.fit_transform(y)
norm_y = norm_y.reshape(1, -1)[0]

"""## Split Dataset

Melakukan pembagian data menjadi 2 bagian yaitu data training dan data test
"""

x_train, x_val, y_train, y_val = train_test_split(x, norm_y, test_size=0.2, random_state=123)

def create_dataset(x, y, batch_size, buffer_size=None, shuffle=True):
  df = tf.data.Dataset.from_tensor_slices((x, y))

  if shuffle:
    df = df.shuffle(buffer_size)

  df = df.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)

  return df

batch_size = 128
buffer_size = len(x)

train_df = create_dataset(x_train, y_train, batch_size, buffer_size)
val_df = create_dataset(x_val, y_val, batch_size, shuffle=False)

"""# Modelling"""

from tensorflow.keras import layers
class RecommenderNet(tf.keras.Model):
  def __init__(self, num_users, num_books, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)

    self.num_users = num_users
    self.num_books = num_books
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-3),
    )
    self.user_bias = layers.Embedding(num_users, 1)
    self.books_embedding = layers.Embedding(
        num_books,
        embedding_size,
        embeddings_initializer='he_normal',
        embeddings_regularizer=keras.regularizers.l2(1e-3),
    )
    self.books_bias = layers.Embedding(num_books, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:, 0])
    user_bias = self.user_bias(inputs[:, 0])
    books_vector = self.books_embedding(inputs[:, 1])
    books_bias = self.books_bias(inputs[:, 1])

    dot_user_books = tf.tensordot(user_vector, books_vector, 2)

    x = dot_user_books + user_bias + books_bias

    return tf.nn.sigmoid(x)

embedding_size = 32

model = RecommenderNet(user_num, book_num, embedding_size)
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = tf.keras.optimizers.Adam(),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

"""# Latih Model"""

history = model.fit(
  train_df,
  epochs = 10,
  validation_data = val_df,
  verbose=1,
)

"""# Get Book Reccomendation
Mencari untuk mendapatkan buku rekomendasi berdasarkan penilaian pengguna
"""

books_df = book_new.drop(['Num-Ratings', 'Book-Author'], axis=1)
rating = pd.read_csv('/content/Ratings.csv')
 
user_id = rating['User-ID'].sample(1).iloc[0]
book_choosen_by_user = rating[rating['User-ID'] == user_id]

book_no_choosen = books_df[~books_df['ISBN'].isin(book_choosen_by_user['ISBN'].values)]['ISBN']
book_no_choosen = list(
    set(book_no_choosen).intersection(set(book2encoded.keys())))
 
book_no_choosen = [[book2encoded.get(x)] for x in book_no_choosen]
user_encoder = user2encoded.get(user_id)
user_book_array = np.hstack(
    ([[user_encoder]] * len(book_no_choosen), book_no_choosen))

ratings = model.predict(user_book_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_books_ids = [
    encoded2book.get(book_no_choosen[x][0]) for x in top_ratings_indices
]
 
print(f'Showing recommendations for users: {user_id}')
print('===' * 5)
print('Books with high ratings from user')
print('----' * 7)
 
top_book_user = (
    book_choosen_by_user.sort_values(
        by = 'Book-Rating',
        ascending=False
    )
    .head(5)['ISBN'].values
)
 
books_df_rows = books_df[books_df['ISBN'].isin(top_book_user)]
for row in books_df_rows.itertuples():
    print(f'{row[1]} ({row[3]})')
 
print('----' * 7)
print('Top 10 book recommendation')
print('----' * 7)
 
recommended_book = books_df[books_df['ISBN'].isin(recommended_books_ids)]
for row in recommended_book.itertuples():
    print(f'{row[1]} ({row[3]})')

"""## Evaluation"""

plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['root_mean_squared_error', 'val_root_mean_squared_error'])
plt.show()